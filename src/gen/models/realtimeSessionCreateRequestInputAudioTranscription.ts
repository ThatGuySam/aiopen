/* eslint-disable jsdoc/check-alignment */

/**
 * Generated by orval v7.5.0 üç∫
 * Do not edit manually.
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 * OpenAPI spec version: 2.3.0
 */

/**
 * Configuration for input audio transcription, defaults to off and can be  set to `null` to turn off once on. Input audio transcription is not native to the model, since the model consumes audio directly. Transcription runs  asynchronously through [OpenAI Whisper transcription](https://platform.openai.com/docs/api-reference/audio/createTranscription) and should be treated as rough guidance rather than the representation understood by the model. The client can optionally set the language and prompt for transcription, these fields will be passed to the Whisper API.

 */
export interface RealtimeSessionCreateRequestInputAudioTranscription {
    /**
 The model to use for transcription, `whisper-1` is the only currently
supported model.
 */
    model?: string
    /**
 The language of the input audio. Supplying the input language in
[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format
will improve accuracy and latency.
 */
    language?: string
    /**
 An optional text to guide the model's style or continue a previous audio
segment. The [prompt](/docs/guides/speech-to-text#prompting) should match
the audio language.
 */
    prompt?: string
}
